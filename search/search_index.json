{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Check My Complete Profile here</p> <p> </p> <p>I am Purnasai Gudikandula. I work as a Senior Data scientist. </p> <p>In a world marked by relentless competition, the pursuit of excellence is ubiquitous. Every individual traversing their career path accumulates a wealth of knowledge\u2014be it in project management, pipeline construction, architectural design, product development, and beyond.</p> <p>Observing diverse approaches to preserving these insights, I've curated a singular repository for all my learning experiences\u2014a place I fondly refer to as 'Valut'.</p> <p>This valut keeps continously updating...!</p> <p>Thank you for checking this page..</p>"},{"location":"AWS/","title":"AWS","text":"<p>This tab houses information about AWS Cloud services. useful services for Data scientists and more.</p>"},{"location":"DB/","title":"PostgreSQL","text":"<p>This Tab holds all the commands, queries like create, rename, delete, join, truncate and more functionalties used in DB handling.</p> <p>We will particularly concentrate on POSTGRESQL.</p>"},{"location":"DL/","title":"DL","text":"<p>This has normal neural network working</p> <p>for ml data classification and regression</p>"},{"location":"DL/Efficient_DL/","title":"Efficient DL","text":"<p>Efficient deep learning with Quantization, Pruning and more</p> <p>keep that Efficient Deep learning book here or contents from it.</p> <p>https://hanlab.mit.edu/courses/2023-fall-65940 https://youtu.be/tvnK93pd810?t=4550 &lt;- at this time stamp, there are recommendations to keep computation faster. not just that, the whole talk is awesome.&gt;</p> <p>The quantization part here from this time stamp: https://youtu.be/tvnK93pd810?t=6709</p>"},{"location":"DL/bytes/","title":"Bytes","text":"<p>For short observations like.</p> <p>Word2vec embeddings are context independent where as transformer embeddings are Context dependent. This understanding of Context gave birth to BERT, Roberta, Longformers, T5, GPT.</p> <p>GPT is a Decoder-Model whose taks is only to predict next word. But no one thought that increasing the Training Data to these models helps them to understand more better &amp; with simple finetuning they can generate task related texts with Context in mind. Excellent, isn't it..?</p> <p>Deep learning is moving from Prgrammable to Configurable with Config files.</p> <p>Some times it is just the architecture that matters most. like - in LLMs, Attention  replaced with flash attention and then with paged-attention from vLLMs. - in transformers for Vision, VIT -&gt; DEIT-&gt; Swin transformer and more.</p> <p> The filters over the Channels is built differently from model to model. - here</p> <p>From My observations with LLMs, Mistral is the Best model so far for me for the year 2023. It has performed balancing output generation for my usecases even when compared with LLama.</p>"},{"location":"DL/bytes/#precision-types","title":"Precision Types:","text":""},{"location":"DL/bytes/#full-precision","title":"Full Precision:","text":"<ul> <li>Format: Typically represented as 32-bit floating-point numbers (float32).</li> <li>Precision: High precision, providing accurate representation of decimal numbers.</li> <li>Advantage: Accurate but requires more memory and computational resources.</li> <li>Common Usage: Default format for many deep learning models.</li> </ul>"},{"location":"DL/bytes/#half-precision","title":"Half Precision:","text":"<ul> <li>Format: Represented as 16-bit floating-point numbers (float16).</li> <li>Precision: Lower precision compared to full precision, but with a smaller memory footprint.</li> <li>Advantage: Reduced memory usage and faster computation, suitable for scenarios with memory constraints.</li> <li>Common Usage: Often used in scenarios where memory and computation efficiency are critical, such as on-device or edge devices.</li> </ul>"},{"location":"DL/bytes/#mixed-precision","title":"Mixed Precision:","text":"<ul> <li>Combination: Involves using a combination of both full precision and half precision in a single model.</li> <li>Usage: Typically, the model's layers are partitioned, and some layers use full precision while others use half precision.</li> <li>Advantage: Balances the benefits of reduced memory usage with the need for high precision in certain parts of the model.</li> <li>Common Usage: Frequently employed in training deep learning models, especially in architectures like mixed-precision training to accelerate training time while maintaining numerical stability.</li> </ul>"},{"location":"DL/gradient_checkpoint/","title":"Gradient checkpoint","text":"<p>gradient accumulation early stopping and more</p>"},{"location":"DL/normalization/","title":"Normalization","text":"<p>layer normalization batch normalization and more</p>"},{"location":"DL/trackers/","title":"Trackers","text":"<p>weights and biases neptune mlflow</p>"},{"location":"DL/understand_torchinfo/","title":"Understand torchinfo","text":"<p>understand torchinfo or torchsummary package. how it is calculating trainable parameters is very important.</p> <p>https://pypi.org/project/torchinfo/</p> <p></p> <p>Like in above picture, do calculate that as well.</p>"},{"location":"DL/understand_torchprofile/","title":"Understand torchprofile","text":"<p>Very Very excellent.</p>"},{"location":"ML/","title":"ML","text":"<p>This houses traditional ML, regularizers, encodings, ensemblings, boosting and more.</p>"},{"location":"ML/Encodings/","title":"Encodings","text":"<p>Simplyfy below from ChatGPT</p> <p>One-Hot Encoding (OHE): This method creates binary columns (0 or 1) for each category in a categorical feature. It is suitable for categorical features with a small number of unique values, such as the column with 6 categorical values in your dataset. OHE is easy to implement and can work well with various machine learning algorithms, including linear models and tree-based models.</p> <p>Label Encoding: This method assigns a numerical label to each category in a categorical feature. It is suitable for ordinal categorical features where the categories have a meaningful order, but may not be appropriate for nominal categorical features where the categories have no inherent order. Label encoding is useful when dealing with a large number of unique values in a categorical feature, such as the column with 35 categorical values in your dataset. However, be cautious when using label encoding with algorithms that assume numerical values have ordinal relationships, as it may introduce unintended biases.</p> <p>Binary Encoding: This method combines aspects of both one-hot encoding and label encoding. It represents each category in a categorical feature with a binary code, which reduces the number of columns compared to OHE, but still captures the unique values. Binary encoding can be useful when dealing with moderate-sized categorical features, such as the column with 15 categorical values in your dataset.</p> <p>Count Encoding: This method replaces each category in a categorical feature with its frequency (or count) in the dataset. It can be useful when dealing with high-cardinality categorical features, such as the column with 20 categorical values in your dataset.</p> <p>Target Encoding: This method replaces each category in a categorical feature with the mean of the target variable for that category. It can be useful for improving the predictive power of a model when there is a relationship between the categorical feature and the target variable.</p> <p>From Chatgpt: Simply this</p> <p>For a column with 6 categorical values: One-Hot Encoding (OHE) is typically suitable. For a column with 20 categorical values: Count Encoding or Target Encoding may be appropriate. For a column with 15 categorical values: Binary Encoding could be a good choice. For a column with 35 categorical values: Binary Encoding or Target Encoding might be suitable.</p>"},{"location":"ML/Explainability/","title":"Explainability","text":"<p>keep shap, and more tools here</p>"},{"location":"ML/algorithms/","title":"Algorithms","text":"<p>classification, regression and clustering.</p>"},{"location":"ML/automated_ml/","title":"Automated ml","text":"<p>Algos or packages like: - Tpot - AutoML - H2O - Auto-Sklearn - Pycaret</p>"},{"location":"ML/feature_selection/","title":"Feature selection","text":"<p>PCA for dimensionality reduction</p> <p>for 10 features in our dataset, after we apply PCA, it gives us 10 Reformulated features, in which most cases, First 2 to 3 feaures would have covered the essence of all features.</p> <p>we still have those remaning 7 features of PCA, which might not retain much informations.</p>"},{"location":"ML/hyperparameter_tune/","title":"Hyperparameter tune","text":"<p>optuna here https://optuna.org/</p> <p>hyperopt</p> <p>sklearn plain grid-search </p> <p>auto-sklearn</p>"},{"location":"ML/lifecycle/","title":"Pipeline","text":"<p>Machine Learning Model Development: Design, develop, and implement machine learning models and algorithms to solve complex problems in automation and Chrome development, such as natural language processing, computer vision, recommendation systems, or predictive analytics.</p> <p>Data Preparation: Collect, preprocess, and analyze data from various sources to create clean and relevant datasets for model training and evaluation.</p> <p>Model Training and Evaluation: Train, fine-tune, and optimize machine learning models using state-of-the-art tools and frameworks. Perform rigorous evaluation and testing to ensure model accuracy, generalization, and performance.</p> <p>Feature Engineering: Identify and engineer relevant features from data, improving model effectiveness and interpretability.</p> <p>Deployment and Integration: Collaborate with software engineers to deploy machine learning models into production systems, ensuring scalability and efficiency.</p> <p>Continuous Learning: Stay up-to-date with the latest developments in the field of machine learning and artificial intelligence. Apply cutting-edge research and technologies to solve business challenges.</p> <p>Collaboration: Work closely with cross-functional teams, including data scientists, software engineers, and domain experts, to understand project requirements, define objectives, and deliver impactful solutions.</p> <p>Documentation: Maintain detailed documentation of machine learning models, data, and processes to facilitate knowledge sharing and reproducibility.</p>"},{"location":"MLOPS/","title":"MLOPs","text":"<p>This tab holds MLOPS related Notes from Available online resources.</p>"},{"location":"MLOPS/Docker/","title":"Part1","text":""},{"location":"MLOPS/Docker/#docker","title":"Docker","text":"<p>Docker makes Depolyment Easy and Reproducible by using Containers.</p> <p>Version: </p> <p>What is Dockerfile:</p> <ul> <li> <p>A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image.</p> </li> <li> <p>is essential blueprint/recepie for constructing docker image.</p> </li> <li> <p>It includes instructions such as what base image to use, what dependencies to install, what files to add, and what commands to run when the image is instantiated.</p> </li> <li> <p>Dockerfiles are used to automate the creation of Docker images, ensuring consistency and reproducibility.</p> </li> </ul> <pre><code>FROM python:3.9\n\nWORKDIR /app\n\nCOPY requirements.txt .\n\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nCMD [\"python\", \"app.py\"]\n</code></pre>"},{"location":"MLOPS/Docker/#image-size","title":"Image Size:","text":"<p>File1:  with 48MB. Note that this doesn't come with python and its dependencies installed. hence the less size.  <pre><code>FROM alpine:3.18\n\nWORKDIR /app\n\nCOPY . .\n\nCMD [\"python\",\"app.py\"]\n</code></pre></p> <p>File2:  with 1.21GB. Note that this python and its dependencies installed. also pandas is installed again. hence the Medium size.  <pre><code>FROM python\n\nWORKDIR /app\n\nCOPY . .\n\nRUN pip install pandas\n\nCMD [\"python\",\"app.py\"]\n</code></pre></p> <p>File3: with 644MB. it has basic Ubuntu latest image, update it, install python3 and pip dependencies. Later install pandas only.</p> <pre><code>FROM ubuntu:latest\n\nRUN apt update\n\nRUN apt install python3 python3-pip -y\n\nWORKDIR /app\n\nCOPY . .\n\nRUN pip install pandas\n\nCMD [\"python\",\"app.py\"]\n</code></pre> <p>command to create DockerImage from the above Dockerfile.</p> <ul> <li>Step1: Build the Docker image. try <code>docker-compose build -t imagename .</code> to build the image.  you can check the recently built image using <code>docker image ls</code>.  you can remove the image using <code>docker image rm -f image_name:tag</code>. or <code>docker image rm image-id</code>.</li> <li><code>docker run -p 80:80 imagename:tag</code> to run the image. Doing this spins up new container automatically.</li> <li><code>docker run -it imagename:tag bash</code> to run the bash shell. type <code>exit</code> to get out of the bash.</li> <li>if the process/code in <code>app.py</code> is still running, then you can see container runing live with command <code>docker ps</code>. To see all containers, use <code>docker ps -a</code>.</li> <li>remove unused image using <code>docker image prune -a</code>.</li> </ul> <p>Docker image prune vs docker image rm <code>docker image prune -a</code> is used to remove dangling images (unused images with no tags), while <code>docker image rm</code> is used to remove specific images by their ID or tag, and it can remove both unused images and images in use by containers. </p>"},{"location":"MLOPS/Docker/#what-is-a-container","title":"What is a Container:","text":"<p>Containers allow a developer to package(this package is nothing but an Docker image.) up an application with all of the parts it needs, such as libraries and other dependencies, and ship it all out as one package. With that, the Application can run on Any machine. This gives universal access to run on any machine.</p> <p>Running Docker image creates Docker container.</p> <p>How to create Docker Container: You need an image called DockerImage to create Container. Docker Image is created from Dockerfile which is a text file created by user. refer to <code>docker run</code> command above in Dockerfile commands for dockerimage creation.</p> <p>you can check Container-id from below example image: </p> <ul> <li>if you want to stop the container running, then use command <code>docker stop containerid</code>.</li> <li>if you want to start the existing container, then use <code>docker start container-id</code>.</li> <li><code>docker containers ls</code> only shows running containers.</li> <li><code>docker containers ls -all</code> shows all available containers.</li> <li>if you want to stop the container running, then use command <code>docker stop containerid</code>.</li> <li><code>docker container prune</code> to remove unused containers. <code>-a</code> flag is not required, as we used to prune image above.</li> </ul>"},{"location":"MLOPS/Docker/#what-is-multi-image-container","title":"What is Multi-Image Container:","text":"<p>what is docker-compose.yml file: Container holds a single image or multiple image. one don't need a <code>docker-compose.yml</code> file when it is a single image. <code>docker-compose.yml</code> file is used to create Multiple Images (or) stack (or) layers of Images.</p> <p>Example:</p> <ul> <li> <p>Ubuntu base image</p> </li> <li> <p>Node</p> </li> <li> <p>mongodb</p> </li> <li> <p>redis</p> </li> </ul> <p>Note: Each container runs on diferent port. docker-compose.yml file for single image:</p> <pre><code>version: \"3.3\" # this is docker-compose version\nservices:\n  api:\n    build: .\n    ports:\n      - \"5000:5000\"\n    volumes:\n      - ./data:/app/data\n</code></pre> <ul> <li><code>docker-compose up</code> is to up the image (or) get image running live.</li> <li><code>docker-compose up --build -d</code> Here, <code>-d</code> means <code>Detached</code> mode from terminal. So you don't see logs running in terminal. without flag <code>-d</code>, it runs the logs in terminal.</li> </ul>"},{"location":"MLOPS/Docker/#what-is-port-mapping","title":"What is port mapping","text":"<p>Any application that runs using docker, creates a container. Since container is a layer on top of os, the dependencies inside a container does not match with denpendencies in local. we can say the container is a whole different environment. </p> <p>So to access the application running in that specific container on server(flask,streamlit,fastapi) with some port, we need to map it to the another local port. so we can access the specific application in local (or) in network url as well.</p> <p>example in docker-compose.yml:  <pre><code>- ports\n  - \"5000\":\"5000\" #container port --&gt; local port\n</code></pre></p> <p>example in Dockerfile: <pre><code># exposet the port to local host\nEXPOSE 8000\n</code></pre></p>"},{"location":"MLOPS/Docker/#what-is-host-mapping","title":"what is host mapping","text":"<p>Every server (or) even container runs on host address <code>0.0.0.0</code>. so to access this address in local, we need to use <code>127.0.0.1</code> along with exposed port <code>8000</code>.</p> <p>References: piyush garg docker tutorial 1 piyush garg docker tutorial 2 Techworld with nana docker tutorial patrick tutorial on devcontainers</p>"},{"location":"MLOPS/Docker2/","title":"Part2","text":""},{"location":"MLOPS/Docker2/#docker-network","title":"Docker Network","text":""},{"location":"MLOPS/Docker2/#docker-network_1","title":"Docker Network","text":"<p>A container can communicate with the internet. we can test it using <code>ping google.com</code>.  A docker container uses Host machines Network connectivity using a method called BRIDGE connectivity.</p> <p>we can inspec the network details that is using BRIDGE connectivity.  use the command <code>docker network inspect bridge</code>. This is the default Network connectivity.</p> <p>type <code>docker network ls</code> to see list of network drivers that docker generally communicates with.</p> <pre><code>NETWORK ID     NAME      DRIVER    SCOPE\n326d12796a29   bridge    bridge    local\n627f5ee25ca3   host      host      local\n952ace5dfb0a   none      null      local\n</code></pre>"},{"location":"MLOPS/Docker2/#changing-network-driver","title":"changing network driver.","text":"<p>By default a Docker container communicates with Outside network using Host network connectivity though BRIDGE connectivity method. </p> <p>We can externally change the Connectivity method by chaning driver with --network=host argument along with <code>docker run</code> argument.</p>"},{"location":"MLOPS/Docker2/#no-port-mapping","title":"No port mapping.","text":"<p>Since we changed the connectivity method to host, Docker container directly uses host network capabilities, so as a result Port Mappping is not required i.e, <code>docker run imagename:tag</code> instead of <code>docker run -p 3000:8000 imagename:tag</code>.</p>"},{"location":"MLOPS/Docker2/#create-new-networkdriver","title":"Create New Networkdriver","text":"<p>we can create our own network driver using <code>docker network create -d bridge newnetworkname</code>. once we created this network driver, we can attach our container to this newly created driver. </p> <p>we can keep two container running through thie newly created network driver using commands like:</p> <ul> <li>container1 with <code>docker run --network=newnetworkname imagename:tag</code> and</li> <li>container2 with <code>docker run --network=newnetworkname imagename1:tag</code>.</li> </ul>"},{"location":"MLOPS/Docker2/#volume-mounting","title":"Volume Mounting:","text":"<p>when a container is destroyed, memory reserved for that container also gets destroyed. To avoid that we can use <code>docker volume</code> to setup/mount our Local folder to the Container. </p> <p>So all the files, folders, applications created in this folder in the container, reflects the same contents in the Local folder. This way even if our container is destroyed, contents in the Local folder stay same.</p> <p>we can connect our local folder to/as the container folder using command <code>docker run -v localfolderpath:containerfolderpath imagename:tag</code>. </p> <p>Later when we want to connect this local folder to another container, then again we use <code>docker run -v localfolderpath:containerfolderpath imagename:tag</code>.</p>"},{"location":"MLOPS/Docker2/#docker-multi-stage-builds","title":"Docker multi-stage builds:","text":"<p>This is also an interesting concept, that one should be familiar with. Note on it later. for now check the official documentation here</p>"},{"location":"MLOPS/Git/","title":"Git","text":""},{"location":"MLOPS/Git/#writing-git-ignore","title":"Writing Git ignore:","text":"<ul> <li>gitignore- to avoid files from tracking</li> </ul> <p>after adding files to git, VSCode dulls folders in FileViewer. Do not include \u201c*\u201d after folder path.</p> <p>check more here</p> <pre><code>git revert commit-id #if this opens up a unix editor, then :wq to write and quit.\ngit config --global --list #to check all accounts\n\n# to configure new account\ngit config --global user.name \ngit config --global user.email \n</code></pre>"},{"location":"Math/","title":"Math","text":"<p>This houses all the math related to deeplearning. this not just holds all math, but the math that I only understand and can relate to in deeplearning.</p> <ul> <li>Stats</li> <li>Probability</li> <li>Linear algebra</li> <li>Calculus</li> </ul>"},{"location":"New_CV/","title":"New CV","text":"<p>This houses advanced architectures like transformers and stable diffusion.</p>"},{"location":"New_CV/MultiModels/","title":"MultiModels","text":"<ul> <li>Representation: computer interpretable descriptions of heterogeneous data from multiple modalities.</li> <li>translation: represents process of changing data from one modality to another.</li> <li>alignment: identifying relations between elements from one or more modalities.</li> <li>fusion: process of joining information from 2 or more modalities to perform prediction task.</li> <li>co-learning: goal of transfering knowledge between modalities and their representations.</li> </ul>"},{"location":"New_CV/MultiModels/#architecture","title":"Architecture","text":"<p>In general, multimodal architectures consist of three parts:</p> <ol> <li>Unimodal encoders encode individual modalities. Usually, one for each input modality.</li> <li>A fusion network that combines the features extracted from each input modality, during the encoding phase.</li> <li>A classifier that accepts the fused data and makes predictions.</li> </ol> <p></p> <p>from here</p> <p>Encoder: Encoder extracts features from input data. Different inputs can be passed to different Encoders.  These Encoders comprised of NNs with Non-linear transformations to extract increasingly abstract features from the input data. The outputs from these independent encoders are combined into a single vector.</p> <p>Fusion: combining information from different modalities into a single representation.</p> <p>Usecases of dealing with just 2 modalities:</p> <ul> <li>Retrieval (image &lt;&gt; text) </li> <li>Captioning (image -&gt; text) </li> <li>Generation (text -&gt; image) </li> <li>Visual question answering (image+text -&gt; text) </li> <li>Multimodal classification (image+text -&gt; label) <ul> <li>Better understanding/generation (image+text -&gt; label/text)</li> </ul> </li> </ul> <p>Some popular models: - PaLI\u00a0(Pathways Language and Image model) - DALL-E - \u00a0Stable Diffusion.</p> <p>key innovations: Dall-e - The use of a discrete latent space, which allows the model to learn a more structured and controllable representation of the generated images.  - The model is optimized using a variant of the VAE loss function called the Gumbel-Softmax trick. Stable Diffusion: - Stable Diffusion uses a diffusion process, which involves iteratively adding noise to an initial image and then progressively removing the noise. By controlling the level of noise and the number of iterations, Stable Diffusion can generate diverse and high-quality images that match the input text prompt. - Diffusion uses a contrastive loss function to encourage the generated images to be diverse and distinct from each other. BEIT: - The current state-of-the-art on the NLVR task is reached by\u00a0BEiT-3. It is a transformer-based model that has been pre-trained on large-scale datasets of natural images and texts, such as ImageNet and Conceptual Captions.</p>"},{"location":"New_CV/MultiModels/#from-the-book-here","title":"From the Book here","text":""},{"location":"New_CV/MultiModels/#chapter-1","title":"Chapter-1","text":"<p>Outline:</p> <ul> <li>NLP:<ul> <li>Word embeddings</li> <li>Attention</li> <li>Self-attention</li> <li>Bert</li> <li>Transformers</li> </ul> </li> <li>CV:<ul> <li>Resnet</li> <li>Efficientnet</li> <li>simCLR</li> <li>BYOL</li> </ul> </li> <li>Multimodal base concept models:<ul> <li>GANs</li> <li>VAE</li> <li>Dall-E</li> <li>Glide</li> <li>CLIP, ALIGN, Florence(these belong one group)</li> <li>Flamingo</li> </ul> </li> </ul>"},{"location":"New_CV/MultiModels/#chapter-21-nlp","title":"Chapter-2.1 NLP","text":"<p>2 major challenges were solved in NLP.  - Word Embeddings are designed such a way that they capture the internal representations of words as dense vectors. so words with similar meaning will have similar embeddings. - Another challenge is to map different lengths input and output data points using Encoders-Decoders(seq-seq) - Another is Attention mechanism that help model to concentrate at particular parts in sentence. the Skip connections in transformer to avoid vanishing gradients &amp; parallel processing unlike serial processing in RNNs. Also use of layer normalization(Layer normalization is a method of normalizing the activations of a layer before passing them to the next layer) to avoid vanishing gradients. - simple framework of contrastive learning from visual representations, which is called SimCLR, outperforms previous work on CNNs. - Recent contrastive methods are trained by reducing the distance between representations of different augmented views of the same image (\u2018positive pairs\u2019) and increasing the distance between representations of augmented views from different images (\u2018negative pairs\u2019) is Bootstrap Your Own Latent (BYOL) - Result of Advance architectures from NLP and CV gave rise to DAll-E. </p> <p>Embedding: - An embedding is hereby defined as a vector of floating point values(with the length of the vector being a hyperparameter).  - The values for the embedding are trainable parameters which are learned similarly to a model learning the weights for a dense layer.  and for translation as well, words in other language also follow similar geometrical arrangements in space. here the languages are close.  For example, the accuracy for translations between English and Vietnamese seemed significantly lower. This can be ascribed to both languages not having a good one-to-one correspondence because the concept of a word is different than in English. </p>"},{"location":"New_CV/MultiModels/#bert","title":"BERT:","text":"<p>Only Encoder model. [MASK] token is used. Bi-Directional since context is important to get current word i.e MASKED.</p>"},{"location":"New_CV/MultiModels/#gpt-3","title":"GPT-3:","text":"<p>Only Decoder model, Auto regressively predicts next word. </p>"},{"location":"New_CV/MultiModels/#t5","title":"T5:","text":"<p>always a seq-seq model. Uses both Encoder and Decoder. Decoder is still a language model to predict next/upcoming word i.e Unidirectional. Encoder is a Masked LM to predict the part of input sequence (or) Masked word i.e Bidirectional. </p> <p>Using More data increased Complexity and Model sizes to Billion Parameters and size to GBs and TBs. Use of fine Tuning Techniques after Pre-training like Supervised Finetuning and RHLF.</p>"},{"location":"New_CV/MultiModels/#chapter-22-cv","title":"Chapter-2.2 CV","text":"<p>after alexnet, more CNN layers are added to add depth. So Resnets came up. Later increasing CNN layers width proved to be best approach. finally Increasing Image resolution, &amp; Possibility of Scaling all 3 dimensions of Images led to EfficientNet.  Efficient NET: </p> <p>RESNET: for Vanishing gradients in Resnets, issue resoled by Normalized(kaiming HE) intialization, and Intermediate layer normalizations. </p> <p>Another obstacle was a degradation problem. It occurs when the network depth increases, followed by saturating and then rapidly decreasing accuracy. </p> <p>Residual learning is adopted to every few stacked layers where a building block is defined:</p> <pre><code>                            y=F(x,{Wi})+x   (2.1)\n</code></pre> <p>x and y present the input and output vectors of the layers. Figure\u00a0below visualizes the building block.</p> <p></p> <p>The function\u00a0F(x,{Wi})\u00a0represents the residual mapping that is to be learned.</p> <p>Leaderboard Table:</p> Model top-1 acc. top-5 acc. EfficientNet-B0 / ResNet-50 77.1 / 76 93.3 / 93 EfficientNet-B1 / ResNet-152 79.1 / 77.8 94.4 / 93.8 EfficientNet-B2 80.1 94.9 EfficientNet-B3 / ResNeXt-101 81.6 / 80.9 95.7 / 95.6 EfficientNet-B4 82.9 96.4 EfficientNet-B5 83.6 96.7 EfficientNet-B6 84 96.8 EfficientNet-B7 / GPipe 84.3\u00a0/ 84.3 97\u00a0/ 97 <p>VIT:  image is is reshaped into a sequence of flattened 2-dimensional patches. usually 16\u00d716\u00a0and map them to\u00a0D dimensions with a trainable linear projection to create patch embeddings.</p>"},{"location":"New_CV/MultiModels/#23-contrastive-learning","title":"2.3 Contrastive Learning:","text":"<p>Process of labelling large unlabelled datasets is time-consuming and expensive. researchers assumed it could be solved with Contrastive learning framework. </p> <p>SimCLR: idea is to have 2 copies of same image, to train 2 networks and compared.  problem: is that it dobules the size of dataset, and became expensive and infeasible. </p> <p>BYOL: Bootstrap your own latent is introduced to avoid doubling dataset size like SimCLR. Idea is to bootstrap its own representations of same image.</p>"},{"location":"New_CV/MultiModels/#chapter-3","title":"CHAPTER-3","text":"<p>References:</p> <ul> <li> <p>https://blog.roboflow.com/multimodal-models/</p> </li> <li> <p>https://web.stanford.edu/class/cs224n/slides/Multimodal-Deep-Learning-CS224n-Kiela.pdf</p> </li> <li> <p>https://slds-lmu.github.io/seminar_multimodal_dl/index.html</p> </li> </ul>"},{"location":"New_CV/Object%20Detection/","title":"Object Detection","text":"<p>This place is more for theory about object detection.</p> <p>IOU metric: Most used Dataset is COCO Dataset. We also have pyccotools package to easily inspect COCO dataset related tasks.</p> <p>WE see terms like <code>AP   |  AP50  |  AP75  |  APs   |  APm   |  APl</code> in mesuring Accuracy of this model. - AP: Average precision - AP50: how much will be average precision, if threshold is 50%. - AP75: how much will be average precision, if threshold is 75%. - APs: AP for small objects. - APm: AP for medium-sized objects. - APl: AP for large objects.</p> <p>As per COCO dataset: - Small objects: Those with a bounding box area smaller than 32^2 (32x32 pixels). - Medium objects: Those with a bounding box area between 32^2 and 96^2. - Large objects: Those with a bounding box area larger than 96^2.</p>"},{"location":"New_NLP/","title":"New NLP","text":"<p>This houses all the new advanced architectures from transformers era.</p>"},{"location":"New_NLP/bytes/","title":"Bytes","text":"<pre><code>from llmlingua import PromptCompressor\n\nllm_lingua = PromptCompressor(\"microsoft/phi-2\")\nprompt = \"The Document I provided is a Tax Invoice. It has CHA services Table in the First page. It has Description, Rate, unit, Amount and other columns. It also has 9 Rows. Some services in table are Survey charges for container, lift of charges, Container detention charges and more. Can you retrieve that.\"\ncompressed_prompt = llm_lingua.compress_prompt(prompt, instruction=\"\", question=\"\", target_token=20)\n</code></pre> <p>Prompt compressor would work better when the prompt lenth is more than 2K tokens, I guess. But it didn't work when the token are already minimum.</p> <p>Check the compressed prompt <pre><code>{'compressed_prompt': 'The Document I provided is a Tax Invoice. It has CHA services Table in the First page. It has Description, Rate, unit, Amount and other columns. It also has 9 Rows. Some services in table are Survey charges for container, lift of charges, Container detention charges and more. Can you retrieve that.',\n 'origin_tokens': 65,\n 'compressed_tokens': 65,\n 'ratio': '1.0x',\n 'saving': ', Saving $0.0 in GPT-4.'}\n</code></pre></p>"},{"location":"New_NLP/semantic_similarity/","title":"Semantic similarity","text":"<p>Semantic similarity is measured by different methods: few methods: - Edit Distance - Jaccard Distance - Fuzzy Distance - Semantic search with Embeddings - Multilingual semantic search</p> <p>thank you ymoslem here: https://github.com/ymoslem/Sentence-Similarity/blob/main/Semantic_Search.ipynb</p>"},{"location":"Notes/","title":"Index","text":"<p>This containes the notes/observations from Blogs, youtube talks, youtube videos here.</p>"},{"location":"Notes/finetuning_LLMs/","title":"Finetuning LLM","text":"<p>Why finetuning is expensive?</p>"},{"location":"Notes/finetuning_LLMs/#memory-calculation","title":"Memory Calculation:","text":"<p>in case of LLama model, we need to allocate per parameter:</p> <ul> <li>2 bytes for the weight</li> <li>2 bytes for the gradient</li> <li>4 + 8 bytes for the Adam optimizer states</li> </ul> <p>With a total of 16 bytes per trainable parameter, this makes a total of 112GB(excluding the intermediate hidden states), given that the largest GPU today has only 80GB it makes finetuning hard.</p>"},{"location":"Notes/finetuning_LLMs/#peft-is-the-saviour","title":"PEFT is the Saviour","text":"<p>Parameter Efficient Fine Tuning can drastically reduce the number of trainable parameters in a model. There are lot of Exisiting PEFT Methods. We will see LORA as the most widely adopted PEFT in the Community.</p> <p></p>"},{"location":"Notes/finetuning_LLMs/#lora","title":"LORA","text":"<ul> <li>LoRA decomposes a large weight matrix into two smaller, low-rank matrices (called update matrices). </li> <li>These new matrices can be trained to adapt to the new data while keeping the overall number of changes low. </li> <li>The original weight matrix remains frozen and doesn\u2019t receive any further adjustments.</li> </ul>"},{"location":"Notes/finetuning_LLMs/#advantages","title":"Advantages:","text":"<ul> <li>LoRA makes fine-tuning more efficient by drastically reducing the number of trainable parameters.</li> <li>The original pre-trained weights are kept frozen, which means you can have multiple lightweight and portable LoRA models for various downstream tasks built on top of them.</li> <li>LoRA is orthogonal to many other parameter-efficient methods and can be combined with many of them.</li> <li>The performance of models fine-tuned using LoRA is comparable to the performance of fully fine-tuned models.-- interesting.</li> <li>LoRA does not add any inference latency when adapter weights are merged with the base model.</li> </ul>"},{"location":"Notes/finetuning_LLMs/#note","title":"NOTE:","text":"<p>in Transformer models LoRA is typically applied to attention blocks only.</p> <p>Code snippet: </p>"},{"location":"Notes/finetuning_LLMs/#qlora","title":"QLORA:","text":"<p>QLORA = Quantized Model weights + Low Rank Adapters. </p> <p>Due to the significantly reduced size of the quantized model it becomes possible to generously place low-rank adaptors at every network layer, which together still make up just 0.2% of the original model\u2019s weight memory footprint. </p> <p>In addition to generous use of LoRA, to achieve high-fidelity fine-tuning of 4-bit models, QLoRA uses 3 further algorithmic tricks:</p> <ul> <li>4-bit NormalFloat (NF4) quantization, a custom data type exploiting the property of the normal distribution of model weights and distributing an equal number of weights (per block) to each quantization bin\u2014thereby enhancing information density.</li> <li>Double Quantization, quantization of the quantization constants (further savings).</li> <li>Paged Optimizers, preventing memory spikes during gradient checkpointing from causing out-of-memory errors.</li> </ul>"},{"location":"Notes/finetuning_LLMs/#note_1","title":"NOTE:","text":"<p>An interesting aspect is the dequantization of 4-bit weights in the GPU cache, with matrix multiplication performed as a 16-bit floating point operation. </p> <p>In other words, we use a low-precision storage data type (in our case 4-bit, but in principle interchangeable) and one normal precision computation data type. </p> <p>This is important because the latter defaults to 32-bit for hardware compatibility and numerical stability reasons, but should be set to the optimal BFloat16 for newer hardware supporting it to achieve the best performance.</p>"},{"location":"Notes/finetuning_LLMs/#using-qlora-in-practice","title":"Using QLoRA in Practice:","text":"<pre><code>import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"facebook/opt-125m\"\n# For LLM.int8()\n# model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True)\n\n# For QLoRA\nmodel = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True)\n</code></pre>"},{"location":"Notes/finetuning_LLMs/#memory-calculation-after-qlora","title":"Memory Calculation After QLoRA:","text":"<p>with QLORA, we need to allocate per parameter:</p> <ul> <li>~0.5 bytes for the weight</li> <li>2 bytes for the gradient</li> <li>4 + 8 bytes for the Adam optimizer states</li> </ul> <p>Giving a total of 14 bytes per trainable parameter times 0.0029 as we end up having only 0.29% trainable parameters with QLoRA, this makes the QLoRA training setup cost around 4.5GB to fit, but requires in practice ~7-10GB to include intermediate hidden states which are always in half-precision (7 GB for a sequence length of 512 and 10GB for a sequence length of 1024).</p> <p>How to train QLoRA using HuggingFace PEFT: </p>"},{"location":"Notes/finetuning_LLMs/#references","title":"References:","text":"<ul> <li>Colab: here</li> <li>Blog: here</li> </ul>"},{"location":"Notes/know_train_deploy_LLMs/","title":"know train depoly of LLMs","text":"<p>Chatbot Arena is a benchmarking platform after Open LLM Leaderboard. In chatbot Arena users rank models based on their responses to the users questions.</p> <p>Like Andrej karpathy mentioned in his talk here, Training of LLMs involves 4 steps, which later became 3 Major Steps:</p> <ul> <li>Pretraining - To give model ability to Generate Next word. It Generally need Entire Internet data to understand the world(words and their relation with next word). Since Training from Scratch needs Gaint GPUs.</li> <li> <p>Supervised Fine Tuning(SFT) - To give model a context (or) tuning it to Custom usecase. TRL library is used for it. It include support for Deepspeed, PEFT(Parameter Efficient Finetuning) with QLORA(Quantized Low Rank Adapter), Multi-GPU processing and more. UNSLOTH as another Option.</p> <p>Ultrachat 200k is one of the Best datasets used for SFT. One of the Best 7B model i.e Zephyr 7B is trained on its parent dataset called Ultrachat. </p> </li> <li> <p>Human Preference training - To make Chatbot from SFT friendly, safe and helpful with Human Preference, which is also called RLHF(Reinforcement Learning with Human Feedback). TRL package can be used here as well. We use a techinque called DPO(Direct Preference optimization). PPO is also one more method. HH-RLHF is one such dataset to use here. Refer to Huggingface DPO documentation here.</p> </li> </ul> <p>Alignment Handbook: is one documented explanation of How huggingface trained Zephyr-7B model. link here</p> <p>GPUS:</p> <ul> <li>Runpod.io</li> <li>vast.ai</li> </ul> <p>Deployment:</p> <ul> <li> <p>Serverless: is that the Same GPU is provided/shared with Other users. These providers charge very minimal per token. Some providers below:</p> <ul> <li>Together.ai</li> <li>Anyscale</li> <li>perplexity.ai</li> </ul> <p>Like mentioned in the OpenAI document here as shown below:</p> <p><pre><code>from openai import OpenAI\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\nmodel=\"gpt-3.5-turbo\",\nmessages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n    {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n]\n)\n</code></pre> It is kind of Sending the Https request to the API.</p> </li> <li> <p>Dedicated: GPU is only allocated for you. Packages like </p> <ul> <li>TGI(from Huggingface), </li> <li>HF Endpoints where you can host your private model or model from hugginface for minimal costs. </li> <li>VLLM is also an opensource provider with Paged attention mechanism.</li> </ul> </li> </ul>"},{"location":"Notes/know_train_deploy_LLMs/#references","title":"References:","text":"<ul> <li>Video here</li> </ul>"},{"location":"Old_CV/","title":"Old CV","text":"<p>Old computer vision methods using opencv, scikit-image, pillow and more. it also houses all pre-processing and post-processing realted to both     - document images     - plain images.</p>"},{"location":"Old_NLP/","title":"Old NLP","text":"<p>NLP methods before transformers and basic building blocks of nlp</p>"},{"location":"Old_NLP/#concepts","title":"Concepts:","text":"<ul> <li>0.Preprocessing</li> <li>1.Encodings</li> <li>2.Tokenizers</li> <li>3.One-gram/Bi-gram/N-gram</li> <li>4.BOW/Word2vec/tf-idf</li> <li>5.Embeddings</li> <li>6.Distances/similarity</li> <li>7.Text Decoding</li> <li>8.POS/NER</li> </ul>"},{"location":"Old_NLP/#usecases","title":"Usecases:","text":"<ul> <li>1.Document similarity</li> <li>2.Document clustering/Topic Modelling</li> <li>3.NER</li> </ul> <p>Will add more...</p> <p>&lt;!--https://www.analyticsvidhya.com/blog/2020/01/3-important-nlp-libraries-indian-languages-python/ https://indicnlp.ai4bharat.org/pages/indicnlp-resources/ https://www.newscatcherapi.com/blog/ultimate-guide-to-text-similarity-with-python https://copyprogramming.com/howto/python-find-similar-words-from-a-list-python</p>"},{"location":"Old_NLP/preprocessing/","title":"Pre-processing","text":"<p>stopwords, stemming, lemmatization, and more. search for this more here</p>"},{"location":"Old_NLP/tokenizers/","title":"Tokenizers","text":""},{"location":"Others/Api_Implementation/","title":"API Implementation","text":""},{"location":"Others/Api_Implementation/#rest-api-framework","title":"REST API Framework:","text":"<ul> <li>Fast api</li> <li>Flask</li> <li>Djano Rest Framework**</li> </ul>"},{"location":"Others/Api_Implementation/#containerization","title":"Containerization:","text":"<ul> <li>Docker </li> <li>Kubernetes</li> </ul>"},{"location":"Others/Api_Implementation/#quick-build-apps","title":"Quick build apps:","text":"<ul> <li>Gradio here: https://modelz.ai/blog/gradio-docker</li> <li>streamlit</li> </ul>"},{"location":"Others/Api_Implementation/#backend-tech","title":"BackEnd tech:","text":"<ul> <li>python(django, flask)</li> <li>node js</li> <li>go lang</li> <li>next js</li> </ul>"},{"location":"Others/Api_Implementation/#frontend-tech","title":"FrontEnd tech:","text":"<ul> <li>angular js</li> <li>react js</li> <li>next js**</li> </ul>"},{"location":"Others/Api_Implementation/#api-development-testing-tools","title":"API Development Testing tools:","text":"<ul> <li>Insomnia</li> <li>postman</li> </ul>"},{"location":"Others/Api_Implementation/#project-design-tool","title":"Project Design tool:","text":"<ul> <li>figma - for ui screen design</li> </ul>"},{"location":"Others/Api_Implementation/#database-management","title":"Database Management:","text":"<p>Understanding databases is crucial. Common ones include: - SQL databases: Such as PostgreSQL, MySQL, SQLite - NoSQL databases: Like MongoDB, Cassandra, Redis.</p>"},{"location":"Others/Api_Implementation/#version-control-system","title":"Version Control System:","text":"<ul> <li>Git: Essential for code versioning, collaboration, and tracking changes</li> </ul>"},{"location":"Others/Api_Implementation/#curl","title":"Curl","text":"<p>curl (short for \"Client URL\") is a command line tool that enables data transfer over various network protocols.  Example curl command:</p> <pre><code>curl -X 'POST' \\\n  'http://localhost:5000/predict' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"image_url\":\"https://images.pexels.com/photos/2071882/pexels-photo-2071882.jpeg?cs=srgb&amp;dl=pexels-wojciech-kumpicki-2071882.jpg&amp;fm=jpg\"}'\n</code></pre> <p>Once you write the code in such a way, that they accept <code>get</code> and <code>post</code> requests using <code>Flask Routes</code> or <code>FastAPI Routes</code>, then we can run this File/application to serve requests.</p> <p>To serve these applications thorugh requests (or) to start serving this File/application we need   <code>Gunicorn</code> or <code>uvicorn</code> like Server Interfaces.</p>"},{"location":"Others/Api_Implementation/#gunicorn","title":"Gunicorn:","text":"<p>is a <code>WSGI(Web Server Gateway Interface)</code> that handles requests to the application in a synchronous(one at a time) manner. Like a Single Lane road in Layman terms.</p>"},{"location":"Others/Api_Implementation/#uvicorn","title":"Uvicorn:","text":"<p>is a <code>ASGI(Asynchronous Server Gateway Interface)</code> that handles requests to the application in a Asynchronous(many at a time) manner. Like a Multi-lane road in Layman terms.</p>"},{"location":"Others/Api_Implementation/#note","title":"NOTE:","text":"<p>Let's say both unicorn and gunicorn are for hosting a web server.</p>"},{"location":"Others/Api_Implementation/#supervisor","title":"supervisor:","text":"<p>is used to manage &amp; monitor a process, ensure they stay running like <code>docker</code> does. Let's say you have a Python script named <code>my_script.py</code> that needs to be constantly running as a background process.</p> <p>one need to create a <code>Configuration</code> file for that. sample one looks as below:  <code>ini command=/usr/bin/python3 /path/to/my_script.py autostart=true autorestart=true stderr_logfile=/var/log/my_script.err.log stdout_logfile=/var/log/my_script.out.log</code></p>"},{"location":"Others/Api_Implementation/#rqworker","title":"RQworker:","text":"<p>RQ (Redis Queue) is a Python library for queuing and processing tasks asynchronously.  They are to handle background job processing in a distributed system.</p> <p>Example scenarios:   - Handling tasks that can be performed asynchronously, such as sending emails or processing image uploads.   - Running background jobs in a Flask or Django application to avoid blocking the main request-response cycle.</p>"},{"location":"Others/Api_Implementation/#ports","title":"Ports:","text":"<p>127.0.0.1: This is the specific numerical IP address of the loopback interface. It also refers to the local machine itself. Similar to <code>localhost</code>, a service set to listen on 127.0.0.1 is only reachable from the same machine where it's running.</p> <p>0.0.0.0: When a service is set to listen on 0.0.0.0, it means it's binding to all available network interfaces on the machine. It allows the service to be accessible from any IP address associated with the host machine, including connections from the local machine (localhost/127.0.0.1) and external devices on the network.</p>"},{"location":"Others/Api_Implementation/#static-file-server","title":"Static file Server","text":"<ul> <li>python -m http.server 8080: is to serve static files. This is only suitable for Local development or Sharing files within your local network.   Ex: if you want to serve files in your ec2 instance machine, then using the above would work. but running this in your local machine &amp; if you want to serve these local files, then this wouldn't work.</li> </ul>"},{"location":"Others/Api_Implementation/#ngrok","title":"NGrok:","text":"<ul> <li>is a Tunneling service. It exposes local host servers to public internet. It allows you to temporarily expose a local server to the internet, which can be useful during development and testing.</li> <li>sign up a free account.</li> <li>install ngrok</li> <li><code>./ngrok http YOUR_PORT</code>    These are arbitrary steps. check internet for more.</li> </ul>"},{"location":"Others/Api_Implementation/#custom-domain","title":"Custom Domain:","text":"<p>using Nginx</p>"},{"location":"Others/Api_Implementation/#ngroknginx","title":"Ngrok/Nginx:","text":"<p>tools for networking and server configurations. Ngrok creates secure tunnels to local servers(localhost:8000 port), allowing access from the internet. Nginx is a web server that manages web traffic, handles reverse proxy, and serves web content efficiently.(Serverconfiguration)</p> <p>Check this link for Customdomain setup here.</p>"},{"location":"Others/Api_Implementation/#background-tasks","title":"Background Tasks:","text":"<ul> <li>Celery (Python): Enables you to run asynchronous tasks and is commonly used for handling background jobs.</li> </ul>"},{"location":"Others/Api_Implementation/#error-monitoring-and-logging","title":"Error Monitoring and Logging:","text":"<p>Sentry, ELK Stack (Elasticsearch, Logstash, Kibana): Tools to monitor and log errors, system performance, and application behavior</p>"},{"location":"Others/Api_Implementation/#caching","title":"Caching:","text":"<p>Redis: Used as an in-memory data store for caching frequently accessed data, improving application performance.</p>"},{"location":"Others/Api_Implementation/#websockets-for-real-time-communication","title":"WebSockets (for real-time communication):","text":"<p>Socket.io (for Node.js): Enables real-time bidirectional event-based communication</p>"},{"location":"Others/Api_Implementation/#authentication-and-authorization","title":"Authentication and Authorization:","text":"<ul> <li>JWT (JSON Web Tokens): For secure authentication</li> <li>OAuth: Authentication framework used for authorization</li> </ul>"},{"location":"Others/annotation_tools/","title":"Annotation tool","text":"<p>This is a free annotation tool from Piotr Skalski, now in roboflow.</p> <ul> <li>annotation tool: Makesense</li> <li>labelimg</li> </ul> <p>add more annotation tools here..</p>"},{"location":"Others/cicd/","title":"Cicd","text":"<p>check ci/cd pipeline here: https://github.com/purnasai/Sample_CICD</p>"},{"location":"Others/docker/","title":"Docker","text":"<p>Docker packs all your excutable code along with its util functions and many more. It is a complete package. This package is called a container. This container can be moved to any application, tools, software, package since it has all that needs to run an end-to-end appliations.</p> <p>This Container itself is an Environment that all a Application needs to run on any machine.</p> <p>This containerization &amp; advantage of using this whole package any system, avoids the problem of \"it runs in my machine, I am not sure why it is not running..\".</p> <p>This Container builds on top of a small base environment i.e called an Image. Users can add many other dependencies or packages on top of a base environment/Image.</p> <ul> <li> <p>Dockerfile: To build the container you need a Dockerfile. It is a text file with a set of instructions used to build a Docker image. </p> </li> <li> <p>Docker-compose.yml: A docker-compose.yml file is used to define and manage multi-container Docker applications. we can add multiple containers here. It is a YAML file that allows you to specify multiple services, their configurations, networks, volumes, and dependencies in a single file. Some key components of a docker-compose.yml file include:</p> </li> </ul> <pre><code>version: '3.8'\n\nservices:\n  web:\n    image: nginx:latest\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n    depends_on:\n      - api\n\n  api:\n    image: node:14\n    working_dir: /usr/src/app\n    volumes:\n      - ./api:/usr/src/app\n    command: npm start\n</code></pre>"},{"location":"Others/frameworks/","title":"Frameworks","text":""},{"location":"Others/frameworks/#frameworks","title":"Frameworks:","text":"<p>Frameworks provide the functionality to perform the following operations for developing web applications.</p> <ul> <li>URL Routing</li> <li>Input form handling and validation</li> <li>Output Formats with Templating Engine HTML, XML, JSON</li> <li>Data Connection, manipulating using ORM mappers</li> <li>Web security</li> <li>Session Storage and retrieval</li> </ul>"},{"location":"Others/frameworks/#advantages-of-python-frameworks","title":"Advantages of Python Frameworks:","text":"<ul> <li>Open source - no need of spending money</li> <li>Good documentation - learn any functionality &amp; key features</li> <li>Security - secured against malicious attacks</li> <li>Integrations and efficiency</li> </ul>"},{"location":"Others/frameworks/#disadvantages","title":"Disadvantages:","text":"<p>Open source code is going to be public, library and framework we have different rules and regulations.</p>"},{"location":"Others/frameworks/#why-python-frameworks","title":"Why Python frameworks?","text":"<ul> <li>Easier Implementation</li> <li>Maintenance</li> <li>Code Re usability</li> <li>Readability</li> </ul>"},{"location":"Others/frameworks/#top-python-frameworks","title":"Top Python Frameworks:","text":"<ul> <li>Flask</li> <li>Django</li> <li>pyramid</li> </ul> <p>By far, the most popular framework according to the trends is Django because the count of enterprises using Django is pretty high. Django's features and characteristics are similar to the business requirements of many industries which might be the reason behind its popularity.</p>"},{"location":"Others/if%20__name__%3D%3D__main__/","title":"If   name  ==  main","text":"<ul> <li>used in python, used at the end of a file.</li> <li>This line in .py file seperates executable code and utility functions code.</li> </ul> <pre><code># Example module.py\n\ndef some_function():\n    # Code for a specific function\n\nif __name__ == \"__main__\":\n    # Code that should only run when this script is executed directly\n    print(\"This will only execute when module.py is run directly\")\n    # You might put some testing code or initialization code here\n</code></pre> <p>It's not mandatory to include <code>if __name__ == \"__main__\":</code> in every Python file, but it's a good practice for creating reusable modules and for separating executable code from module definitions.</p>"},{"location":"Others/notes_taking/","title":"Notes Taking","text":"<p>These are some of the Notes taking platforms I do use:</p> <ul> <li>Notion - for storing resouces, checklist, plans and more.</li> <li>Obsidian - purely for Notes taking.</li> <li>Miro/Excalidraw - for freehand designs.</li> </ul>"},{"location":"Others/observations/","title":"Observations","text":"<p>Must not forget that Detectron2, Pytorch-lightning, Hugging-face transformers, and Pytorch has option to resume training.</p> <p>With this resume training option, we can continuously train on colab. </p> <p><code>image.shape</code> works in opencv     - outputs (height, width, channels)</p> <p><code>image.size</code> works in PIL(pillow)     - outputs (width,height).     - img.mode gives no.of.channels like <code>RGB</code>. then what is torch follows..?</p> <p>if <code>!cd</code> doesn't work in colab, then try <code>%cd</code>. check here</p> <p>Shell command in python code: <pre><code>import subprocess\n\ndef convert_pdf_to_html(pdf_path, html_path):\n    command = f\"pdf2htmlEX {pdf_path} --dest-dir {html_path}\"\n    subprocess.call(command, shell=True)\n\ninput_pdf = \"quarterly-nvidia.pdf\"\noutput_pdf = \"quarterly-nvidia\"\n\nconvert_pdf_to_html(input_pdf, output_pdf)\n</code></pre></p>"},{"location":"Others/terminologies/","title":"Terminologies","text":"<p>This page has all terminologies / nominclature / nuance that we use in Data Science lifecycle.</p>"},{"location":"Others/terminologies/#git","title":"GIT","text":"<ul> <li>staging branch</li> <li>main/master branch</li> <li>test branch</li> <li>push/pull/merge/</li> <li>commit</li> <li>checkout</li> <li>HEAD</li> </ul>"},{"location":"Others/terminologies/#environment","title":"Environment","text":"<ul> <li>Test environment</li> <li>preproduction enviroment</li> <li>production environment</li> <li>staging environment</li> <li>Development server</li> <li>Deployment server</li> </ul>"},{"location":"Others/terminologies/#cmd","title":"CMD","text":"<ul> <li>putty</li> <li>power shell</li> <li>command prompt</li> </ul>"},{"location":"Others/terminologies/#project","title":"Project","text":"<ul> <li>SME-Subject matter expertise</li> <li>SPOC- Single point of contact</li> <li>MVP- Minimum viable product</li> <li>UAT- User acceptance testing</li> <li>SOW- Scope of Work</li> </ul>"},{"location":"Others/terminologies/#deployment","title":"Deployment","text":"<ul> <li>Synchronous(one request at a time)- serial request handler</li> <li>Asynchronous(multi requests at a time)- Parallel requests handler</li> <li>latency(task completion time)</li> <li>throughput(no.of.requests to handle)</li> <li>Deploy/Ship</li> <li>logging</li> <li>monitoring</li> <li>metrics</li> <li>model explainabillity</li> <li>hyper parameter tuning</li> <li>data drift</li> <li>Concept drift (condition to decide churn, now changed over time)</li> <li>model drift</li> <li>Reproducebility</li> <li>trigger based model retraining</li> <li>fixed window size data(from last year to current day)</li> <li>dynamic window size(moving average)</li> <li>Migrate(from pytorch to pytorch_lightning)</li> <li>Integrate(combine all modules)</li> <li>feasibility study</li> <li>Scoping</li> <li>Requirements</li> <li>MVP(Minimum viable product)</li> <li>UAT(user acceptance testing)</li> <li>Model metric(classification &amp; regression)</li> <li>Dependency</li> <li>Resource allocation </li> <li>sprint</li> <li>scrum master</li> <li>work breakdown structure</li> <li>code review</li> <li>release management</li> <li>pip- performance improvement programe.</li> </ul>"},{"location":"Others/terminologies/#business","title":"Business:","text":"<ul> <li>SLA(Service level agreement)- things we promise to deliver as service to client.</li> <li>Business metric(increase in email openings)</li> <li>KPI(key performance indicators)-Quantifiable measures used to evaluate the success or performance of an organization, project, or individual against specific objectives or goals.</li> <li>ROI- return of investment.</li> <li>SWOT- strength, weakness, opportunity and threat.</li> <li>B2c/B2B - business models</li> <li>QA- Quality assurance</li> <li>RFP- Request for proposal/Request for Quote.</li> <li>SOP- standard operating procedure.</li> <li>CRM- Customer relationship manager</li> <li>ERP- Enterprise resource planning.</li> <li>Stakeholder</li> </ul>"},{"location":"Others/terminologies/#models","title":"MODELs:","text":"<p>Knowledge distillation involves training a smaller model (student) to mimic the behavior of a larger, pre-trained model (teacher) like BERT. Distillbert is Smaller &amp; quicker to Bert. This compact model learns not just the teacher\u2019s predictions but also its confidence and reasoning. This approach is particularly useful when deploying BERT on resource-constrained devices.</p> <ul> <li>latency- delay b/n input &amp; output.</li> <li>throughput- no of predictions a model can handle in specific time.</li> <li>zero, one, few shot learning</li> <li>stateless training (from sebastian raschaka Q&amp;A book)</li> <li>statefull training</li> <li>data-centric ai(focus on data to improve performance.)</li> <li>model-centric ai(focus on model to improve performance.)</li> <li>model efficiency:<ul> <li>pin memory</li> <li>num workers</li> <li>gradient checkpoint</li> <li>Model pruning</li> <li>Model quantization</li> <li>Learning rate schedule</li> </ul> </li> <li>label smooting</li> <li>transfer learning</li> <li>Mixed precision training</li> </ul>"},{"location":"Others/terminologies/#startup","title":"Startup","text":"<ul> <li>bootstraped- self funding with no external</li> <li>Vertical SAAS- Building on single usecase indepth- End-to-End</li> <li>Horizontal SAAS- Building on Multiple usecases paralelly like detection, classification, ocr all at once. </li> </ul>"},{"location":"Others/terminologies/#others","title":"Others","text":"<ul> <li>Mutually exclusive: 2 events won't happen simultaneously.</li> <li>Non Mutually Exclusive: 2 events occur same time.</li> </ul>"},{"location":"Others/tesseract_in_colab/","title":"Tesseract","text":""},{"location":"Others/tesseract_in_colab/#install","title":"install","text":"<pre><code>!sudo apt install tesseract-ocr\npip install pytesseract\n</code></pre>"},{"location":"Others/tesseract_in_colab/#import","title":"import","text":"<pre><code>import pytesseract\npytesseract.pytesseract.tesseract_cmd = r'/usr/bin/pytesseract'\n</code></pre>"},{"location":"Others/understanding_gpu/","title":"Gpu understanding","text":"<p>GPU:</p> <p>Get complete GPU driver information here.</p> <p>Below are some of the well know GPU providers. | https://jarvislabs.ai/pricing/ | https://www.e2enetworks.com/pricing | Runpod | | --- | --- | --- | | https://fullstackdeeplearning.com/cloud-gpus/ | https://lambdalabs.com/gpu-benchmarks | Vast.ai | | https://www.paperspace.com/pricing?_gl=11eg3zpd_gcl_au*MTM5MTExMzc5MC4xNjg1MDIxMzI2 | https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/ |  |</p> <p>Remember: Google colab &amp; Kaggle offers free GPU of 16GB, which is Enough for Most experiments and Quick demos.</p> <p>Before Deciding to select a provider, Go through Below link to know more about:     - Types of GPUs available     - Their Memory, VRAM, Storage     - Cost and more</p> <p>link here</p> <p>NOTE: What\u2019s so expensive per an hour might not be expensive for experiment. </p> <p>Different GPU architectures like Kepler, Pascal, Volta, Turing, Ampere with different products like K80, P100, \u2026. A6000. Tensor TFLops has Mixed precision, means 16bit+32bit precision in tensors.</p>"},{"location":"Others/understanding_gpu/#knowing-gpu","title":"Knowing GPU:","text":"<p>GPUs- knowing hardware better has helped to fasten the computing in the LLMs recently. Flash Attention is one such discovery.</p> <p>Data flow: speed increases as we go towards the end of the flow.</p> <p>Hardware \u2192 Ram \u2192 L2/L3 cache \u2192L1 cache/registers in CPU(these are also called onchip memory) \u2192 GPU</p> <p>Let's dig one step ahead in GPU as well. The below is how GPU looks. </p> <p> </p> <p>Tensor cores specialized in matrix multiplication. Cuda cores are from Nvidia. A group of threads is called Threadblock/warps. The warp scheduler handles thread allocation.</p> <p>1ghz mean = 10^9 operations/cycles per second.</p> <p>Tensor cores are so fast that they are always idle for half of the time. Researchers found that while training GPT3, they saw tensor cores being idle for half of the time.</p> <p>A100 is implemented with HBM(High bandwidth technology) by ordering GPU stacks vertically. each GPU stack has 16GB. so 5Stacks*16GB=80GB</p> <p>FLOPS: Floating operations per second.</p>"},{"location":"Others/understanding_gpu/#a100-architecture","title":"A100 Architecture:","text":"<p>Each Green block is one SM(Streaming Multiprocessors)</p> <p> </p> <p>A more closer look into 1 SM(streaming multiprocessors): we can see L1 cache and L0 cache. we also see Warp Scheduler that schedules Threads properly.</p> <p> </p>"},{"location":"project_maintainence/","title":"Project Maintainence","text":"<p>This hold all Information about</p> <ul> <li>The project strcture </li> <li>Reproducibility</li> <li>Process/memory kill</li> <li>Project maintainance and more here.</li> </ul>"},{"location":"project_maintainence/PMP%20course/","title":"PMP course","text":"<p>This page has Notes from the Project Management Professional Course.</p> <ul> <li>PM- stands for Project Management</li> <li>PMO- Project Management office</li> <li>OPM- Organization project management</li> <li>KPI- Key performance Indicators</li> <li>Phasegate- untill a phase is fully completed, I can not move to the next phase in project.</li> </ul>"},{"location":"project_maintainence/PMP%20course/#business-value","title":"Business value:","text":"<p>what are the project success criteria/checklist. projects end when:</p> <ul> <li>Objectives meet</li> <li>Objectives can not or will not be met</li> <li>Funds are depleted</li> <li>Need no longer exist</li> <li>Resources are no longer available.</li> </ul>"},{"location":"project_maintainence/PMP%20course/#projects-create-business-value","title":"Projects create business value:","text":"<ul> <li>Tangible business value:<ul> <li>Monetary assets</li> <li>stockholder equity</li> <li>market share</li> </ul> </li> <li>Intangible business value:<ul> <li>Goodwill &amp; reputation</li> <li>Brand recognition</li> <li>benefit to public</li> <li>strategic alignment</li> </ul> </li> </ul>"},{"location":"project_maintainence/PMP%20course/#projects-initiation-context","title":"Projects initiation Context:","text":"<ul> <li>stakeholder requested for project</li> <li>technological advancement created need for new project</li> <li>crating new products, improving exisiting produts, and more</li> </ul>"},{"location":"project_maintainence/PMP%20course/#typical-project-managements","title":"Typical Project managements:","text":"<ul> <li>identifying requirements</li> <li>addressing needs, concern, rquirements of stakeholders.</li> <li>settingup, maintaining, and carrying communication.</li> <li> <p>balancing competing project constraints.</p> <ul> <li>Scope</li> <li>quality</li> <li>budget</li> <li>resources</li> <li>risks</li> </ul> </li> </ul>"},{"location":"project_maintainence/PMP%20course/#pm-lifecycle","title":"PM LifeCycle:","text":"<ul> <li>Idea or Concept<ul> <li>Formulate the Idea<ul> <li>Business case<ul> <li>feasibility study<ul> <li>project</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"project_maintainence/PMP%20course/#project-management-application-areas","title":"Project Management application areas:","text":"<ul> <li>Construction</li> <li>Healthcare</li> <li>finance</li> <li>IT</li> <li>Govt</li> <li>NGOs</li> </ul>"},{"location":"project_maintainence/PMP%20course/#other-areas-that-effect-project-management","title":"Other areas that effect project Management:","text":"<ul> <li>program management</li> <li>portfolio management<ul> <li>A senior level excutive managing where &amp; how do we invest in programs and different projects.</li> <li>portfolios are about maximizing investments.</li> <li>portfolio is collection of programs, program is collection of projects.</li> </ul> </li> </ul>"},{"location":"project_maintainence/PMP%20course/#pmo-project-management-office","title":"PMO- Project management office:","text":"<ul> <li>supports project managers</li> <li>manages shared resources</li> <li>coaching, mentoring &amp; training for project managers</li> <li>conducts project audits(to ensure whether using right tools)</li> <li>develops &amp; maintains procedures.</li> <li>facilitates communication across different projects to speedup</li> </ul>"},{"location":"project_maintainence/PMP%20course/#organizational-culture-and-structure","title":"Organizational Culture and Structure:","text":"<ul> <li>Vision</li> <li>Mission</li> <li>Values and Beliefs</li> <li>Cultural Norms</li> <li>Hirarchy and authority</li> <li>organizational &amp; managment style</li> </ul> <p>Project Management Process: </p>"},{"location":"project_maintainence/PMP%20course/#work-performance-data","title":"Work Performance Data:","text":"<ul> <li>Raw data &amp; facts about project.</li> <li>status of project work assignments<ul> <li>percent complete</li> <li>in progress</li> <li>start &amp; finish dates</li> </ul> </li> <li>Data can include:<ul> <li>Cost of the activities.</li> <li>no.of.change requests.</li> <li>Defects</li> <li>Duration</li> </ul> </li> </ul>"},{"location":"project_maintainence/PMP%20course/#work-performance-information","title":"Work Performance Information:","text":"<ul> <li>Analyzed work performance data.</li> <li>usable information to make decisions.</li> <li>status of actionable results.</li> </ul>"},{"location":"project_maintainence/PMP%20course/#work-performance-reports","title":"Work Performance Reports:","text":"<ul> <li>Work performance information in communicable form.</li> <li>Status reports</li> <li>Memos</li> <li>Dashboards</li> <li>Project updates</li> <li>helps stackholders make decision</li> </ul>"},{"location":"project_maintainence/PMP%20course/#project-management-lifecycles","title":"Project Management Lifecycles:","text":"<ul> <li>Predictive lifecycle<ul> <li>plan-driven</li> <li>waterfall approach</li> <li>predicts the project life cycle</li> <li>changes to scope are tightly contolled</li> </ul> </li> <li>Iterative (or) Incremental lifecycle<ul> <li>iterations create deliverables(we deliver in each iteration)</li> <li>detailed scope is elaborated for each iteration.</li> <li>changes to the project scope are expected.</li> </ul> </li> <li>Adaptive Life cycle<ul> <li>change driven</li> <li>agile project managment</li> <li>rapid iterations (or) project work (or) sprint based (15 days mostly)</li> <li>backlog of requirements</li> <li>changes to the project scope are expected</li> </ul> </li> </ul> <p>Business Document for Project Management(to present to management):</p> <ul> <li>report phase-gate work</li> <li>report actual performance/costs compared to earlier business documents.<ul> <li>if I did great, then cool</li> <li>if not, then create variance/exception report to report why I did not hit my KPI, why I was late or why I was over budget.</li> </ul> </li> <li>Decision of comparision include:<ul> <li>should we continue to next phase(if overcost, if too much delay,)</li> <li>end of the project</li> <li>should remain in the same phase</li> <li>repeat the phase again</li> </ul> </li> </ul>"},{"location":"project_maintainence/PMP%20course/#project-business-case","title":"Project Business case:","text":"<ul> <li>economic feasibility study- is it financially profitable..?</li> <li>benefits the project creates</li> <li>project sponsor is accountable for the developement &amp; maintainence not the manager.</li> <li>PM responsible for  providing recommendations.</li> </ul>"},{"location":"project_maintainence/PMP%20course/#business-needs","title":"Business Needs:","text":"<ul> <li>what is prompting the need for action</li> <li>statement document the opportunity</li> <li>stakeholders effected</li> <li>identification of the scope</li> </ul>"},{"location":"project_maintainence/PMP%20course/#business-case-project-determination","title":"Business case: project determination:","text":"<ul> <li>root cause of an opportunity</li> <li>gap analysis of capabilities</li> <li>know risks</li> <li>critical success factors</li> <li>Decision criteria</li> </ul>"},{"location":"project_maintainence/PMP%20course/#organizational-knowledge-repositories","title":"Organizational knowledge repositories:","text":"<ul> <li>cataloging</li> <li>archieveing</li> <li>retrievable</li> <li>Organization process Assets(the then worked team on the usecase) are part of knowlege repositories.</li> <li>archieve at closure.</li> <li>storage<ul> <li>project files from past projects</li> <li>historical information and lessons learned</li> <li>issues and defect databases</li> <li>configuration management databases</li> <li>financial databases</li> </ul> </li> </ul>"},{"location":"project_maintainence/PMP%20course/#role-of-project-manager","title":"Role of Project Manager:","text":"<ul> <li>Manage things, lead people to conclusion</li> <li>getting things done</li> <li>active listener &amp; speaker</li> <li>written &amp; oral to internal(team) &amp; to external(to non team and to management)</li> <li>project manage negotaites:<ul> <li>aim for fair agreement</li> <li>priorities</li> <li>technical approach</li> <li>project scope</li> <li>schedule</li> <li>cost</li> <li>changes to project scope, schedule , budget</li> <li>vendor teams and conditions</li> <li>resource constraints.</li> </ul> </li> <li>PMs solve problems<ul> <li>problem definition</li> <li>RCA- root cause analysis</li> <li>treat causes, not symptoms</li> <li>Don't go to the management without a solution</li> </ul> </li> </ul> <p>Values to have to be a PM:</p> <ul> <li>knowledge: understanding project management</li> <li>performance: accomplish as a project manager</li> <li>personal: behavior, effectiveness, character, leadership</li> </ul>"},{"location":"project_maintainence/PMP%20course/#project-documents","title":"Project Documents:","text":""},{"location":"project_maintainence/memory_release/","title":"Memory release","text":"<p>In the current GPU world, Deep learning models either in Computer vision or Natural language processing require lot more GPUs to run/infer on the Data.</p> <p>It is most of times, important to control/limit the GPU usage. here is the sample snippet that we can use to kill the process running in GPU:</p> <pre><code># remove memory taking variables\ndel model\ndel tensor\n\n# List all PyTorch tensors and their sizes currently allocated on GPU\nprint(torch.cuda.memory_summary())\nprint(torch.cuda.memory_allocated())\nprint(torch.cuda.memory_reserved())\n\nray.shutdown()\ngc.collect()\ntorch.cuda.empty_cache()\n</code></pre>"},{"location":"project_maintainence/project_structure/","title":"Project structure","text":"<p>These are the some points that I noticed in my past experience in Data science life cycle with projects.</p> <p>Best project leads to best product: - Using these below mentioned tools/Concepts for Best Practices make a Best Project, leads to Best product.</p>"},{"location":"project_maintainence/project_structure/#cookie-cutter","title":"Cookie cutter","text":"<ul> <li> <p>Every project/client/Organization should have a Standard Template of Repo/Folder like a Cookie-Cutter. Cookie-Cutters comes with Standard Folder structure with </p> <ul> <li>Depedency management tool- poetry,pip.</li> <li>Logging</li> <li>Continous integration - ci/cd through git actions.</li> <li>Pre-commit check</li> <li>Code linters &amp; formaters- ruff</li> <li>Code security- bandit</li> <li>Testing - pytest</li> <li>Documentation-mkdocs</li> <li>Containerization - docker</li> </ul> <p>and more. Reference repo at current time here</p> </li> </ul>"},{"location":"project_maintainence/project_structure/#documentation","title":"Documentation","text":"<ul> <li>Documentation is very important in a project. Documenting each and every step, idea,scope, issues would help any individual to quickly understand the project in detail. One should Document the </li> <li>Project overview</li> <li>Development setup guide</li> <li>Architecture</li> <li>Packages &amp; configurations</li> <li>Code</li> <li>Database schema</li> <li>Run times </li> <li>Metrics</li> <li>Logs</li> <li>Risk rigestor<ul> <li>A document used to record and track identified risks throughout the project lifecycle, including their likelihood, impact, and mitigation strategies.</li> </ul> </li> <li>Testing Documentation</li> <li>Deployment Documentation</li> <li>Code style guide<ul> <li>coding standards and best practices to maintain consistency across the codebase.</li> </ul> </li> <li>Security documentation<ul> <li>like any security concerns, vulnerabilities.</li> </ul> </li> <li>and more at the end of every project.</li> </ul>"},{"location":"project_maintainence/project_structure/#sample-ml-folder-structure","title":"Sample ML folder structure","text":"<pre><code>- input\n    - train.csv\n    - test.csv\n- src\n    - create_folds.py\n    - train.py\n    - inference.py\n    - models.py\n    - config.py\n    - model_dispatcher.py\n- models\n    - model_rf.bin\n    - model_et.bin\n- tests\n    - test_train.py\n    - test_models.py\n    - test_inference.py\n- notebooks\n    - exploration.ipynb\n    - check_data.ipynb\n- README.md\n- LICENSE\n</code></pre>"},{"location":"project_maintainence/project_structure/#vs-code-ide-extensions","title":"VS Code IDE Extensions","text":"<ul> <li>Adding Extensions to your Coding IDE, makes life easier.<ul> <li>Better comments by aaron bond</li> <li>dev containers by microsoft</li> <li>docker by microsoft</li> <li>draw.io integration henning dieterichs</li> <li>Excel viewer grapecity</li> <li>github actions by github</li> <li>Markdown preview enhanced by yiyi wang</li> <li>powershell by microsfot.</li> <li>remotes-ssh by microsoft.</li> <li>vscode-pdf by tomoki1207</li> <li>Tree - for TODO in vs code</li> <li>Mintlify - to add automatic Class documentation   and more</li> </ul> </li> </ul>"},{"location":"project_maintainence/project_structure/#multi-processing","title":"Multi-Processing","text":"<p>Use of Multi-threading, multi-processing concepts for parallization shows project efficient usage in using all avaible resources. </p> <p>I have seen Companies loosing millions due to not using A100 GPUs efficiently(I mean parallelization.) we can reduce the Computing costs heavily.</p> <p>Refer complete Notes on Multiprocessing here</p>"},{"location":"project_maintainence/reproducibility/","title":"Reproducibility","text":"<p>Reproducibility of Deep learning models in accuracy or across other metrics like losses change in Different runs based on the Environments used, software application, hardware infrastructure.</p> <p>Though it is not completely possible to reproduce at lower level like tensors/neurons in model layers, we can reproduce the model metrics to most extent.</p> <pre><code>import torch\nimport numpy as np\nimport random\nfrom transformers import set_seed\n\ndef set_seed_everywhere(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    set_seed(seed)  # For Hugging Face Transformers\n\ndef set_reproducibility(seed=42):\n    set_seed_everywhere(seed)\n\n    # Additional settings for reproducibility in PyTorch\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n    # Set environment variable for better reproducibility on CPU\n    torch.set_deterministic(True)\n\n    # Additional configurations if necessary for specific libraries or modules\n    # For example, for CuDNN enabled devices, disabling CuDNN might be helpful\n    # torch.backends.cudnn.enabled = False\n\n    # You can add other settings specific to your use case here\n\n# Usage:\nset_reproducibility(seed=123)\n# Your code utilizing PyTorch and Hugging Face Transformers models goes here\n</code></pre>"},{"location":"project_maintainence/source_title/","title":"Source title","text":"<p>Source title is must at the beggining of Every code file. be it .py, .java, .c, .ipynb and more.</p> <p>This titles shows the author, date it is created, Description of current file, take aways from it.</p> <pre><code>Title: \n    Notes Utilities\n\nDescription:\n    This file has all the functionalities/utilities required for Notes processing.\n\nTakeaways:\n    - It even processes the Tables in the Coverpage.\n    - Check boxes are not properly parserd. to future scope.\n\nAuthor: purnasai@organizationmail\nDate: 10-10-2023\n</code></pre>"},{"location":"python/","title":"Python","text":"Existing Dependencies  - os  - re  - io  - json  - datetime  - random  - logging  - threading  - multiprocessing  - itertools  - argparse  - collections  - functools  - shutil  - pathlib  - typing  - warnings  Math Dependencies  - math  - scipy  - statsmodels  DataScience Dependencies  - pandas  - numpy  - matplotlib  - seaborn  - plotly  - opencv  - pillow  - scikit-image  - scikit-learn  - NLTK  - pytorch  - transformers"},{"location":"python/Bytes/","title":"Bytes","text":"<p>1.Varibale Casing</p> <p>Casing in python: - Pascal casing is not used in python much. - Snke casing is used mostly. </p> <p>2.id(): unique identifier is memory address of object referenced by a variable.  <pre><code>a = 10000\nid1 = id(a)\nb = a + 2-2\nid2 = id(b)\n\nprint(id1)\nprint(id2)\n</code></pre></p> <p>2.1. <code>ord</code> and <code>chr</code>: <pre><code>unicode_code_point = ord(\"h\")\nprint(unicode_code_point)\n# 104\n\nhuman_readable_character = chr(104)\nprint(human_readable_character)\n# h\n</code></pre></p> <p>3.0. <code>and</code> VS <code>or</code>: <pre><code># inclusive\nif 5&gt;0 and 5&lt;10: # when the number is b/n 2 bounds\n    print(\"Yes Condition true\")\n</code></pre> or  <pre><code># Exlusive\ns = 10\nif 10 &lt; 15 or 10&gt; 0: # when the number is outside 2 bounds\n    print(\"yes it is true\")\n</code></pre> 3.iterating a pair <pre><code>numbers = range(100)\n\n#using zip(numbers, numbers[1:]), iteration wont go out of index\n# zip equals the 2 lists to the list with low length.\nfor i,j in zip(numbers, numbers[1:]): \n    print(i,j)\n</code></pre> 3.copy with out impacting original <pre><code>numbers = list(range(100))\nprint(numbers)\n\n\nnumbers1 = list(numbers)# this creates a copy of original\nnumbers1[0] = 10000\nprint(numbers1)\n</code></pre></p> <p>4.If any problem is related to Arrays &amp; it involves o(n^2) time complexity, then doing a computation that takes O(log n) wont make much difference. So Overall time complxity will again be O(n^2) only.</p> <p>5.Anonymous Variable.</p> <pre><code>for _ in range(100): # here instead of \"i\", \"_\" is used. so we are limiting memory &amp; usage.\n    print(\"This is Bytes of Python\")\n\na,b,_ = \"value1\", \"value2\", \"value3\" # since we dont need \"value3\" to use, so using anonymous variable.\n</code></pre> <p>6.Function readability</p> <p>Instead of keeping multiple logics in Single function, it is always better to keep multiple simple functions for multiple logics. this way it is easier to route through, to understand, to read the code more clearly.</p> <p>7.Type hinting</p> <p>Type hinting in python enhances code readability. helps anyone to easily understand the type of data that goes inside and comes outside of a function.</p> <p> </p> <p>8.Exception-handling</p> <p>Exception handling is must and always handy.</p> <p>9.Parquet using parquet(instead of csv/json/numpy/pickle) file comes with less storage size &amp; quick redability</p>"},{"location":"python/Datastructures/","title":"Data structures","text":"<p>This has all the python course it needs.</p> <p>It has 4 modules:</p> <ul> <li>Data structures: For Basic python</li> <li>OOPS: For Classes and Functions in python</li> <li>Algorithms: For Search, Sort and Other algorithms</li> <li>Bytes: Quick points to Excel</li> </ul>"},{"location":"python/Datastructures/#data-structures","title":"Data Structures:","text":"<ol> <li> <p>Max Number: Finding the Max number in a list <pre><code># problem: Largest of N numbers\n# solution: we can use a for loop, iterate over &amp; compare.\nnumbers = [23,45,56,44,56,77,33,44,22,11,10]\n\nbig = numbers[0] \n# it is advantage to take the \"\"First value\"\" in list, rather than taking big = np.inf\n# or big = 0\n\nfor number in numbers:\n    if number&gt;big:\n        big = number\n\nprint(big)\n</code></pre></p> </li> <li> <p>GCD or HCF: Finding the Max Common number that devides the numbers. <pre><code># Problem: find GCD (or) HCF\n# Solution: get common max number, start dividing from 1st table.\na = 30\nb = 25\n\nbig = 1 # starting with 1st table\nfor i in range(1, max(a,b)): # iterating till to the max value of both vals, to get common number.\n    if a%i == 0 and b%i == 0:\n        if i&gt;big:\n            big = i\n\nprint(big)\n</code></pre></p> </li> <li> <p>Proper use of <code>if</code> vs <code>elif</code>: <pre><code>button = int(input())\n\nwhile button != 6:\n    if button &lt;= 5 and button&gt;=1: # since it is b/n 2 bounds, \"AND\" is used.\n        a = int(input())\n        b = int(input())\n    if button ==1:\n        print(a+b)\n    if button ==2:\n        print(a-b)\n    if button ==3:\n        print(a*b)\n    if button ==4:\n        print(a//b)\n    if button ==5:\n        print(a%b)\n    elif button &lt;1 or button &gt;5: # Since it is outside of 2 bounds, \"OR\" is used.\n        print(\"Invalid Operation\")\n    button = int(input())\n</code></pre></p> </li> <li> <p>Fibanocci number: Fibanocci is a number pattern that use its precident 2 numbers and keeps their sum as 3 rd number. There are many different ways. this is one way. <pre><code>N = 20\nn1 = 1\nn2 = 2\n\nsumm = n1+n2\nflag = True\nwhile flag:\n    if summ &lt; N:\n        print(summ)\n        summ = n1+n2\n        n1 = n2\n        n2 = summ\n    else:\n        flag = False\n</code></pre></p> </li> <li> <p>Reverse Number <pre><code>num = 1234\nreversed_num = 0\n\nwhile num != 0:\n    digit = num % 10 # it gives the remainder. Always last value in a number from right side.\n    reversed_num = reversed_num * 10 + digit # this increases number at 10X &amp; adds Remainder. \n    num //= 10 # this gives the quotient with roundedness.\n\nprint(\"Reversed Number: \" + str(reversed_num))\n</code></pre></p> </li> <li> <p>Palindrome Number: If Reverse of a number equal to the number, then it is palindrome. ex: <code>121</code> <pre><code>n=int(input())  \n\ndef check_palindrome(n):\n    reverse = 0\n    start = n\n    while n:\n        remainder = n%10\n        reverse = (reverse*10) + remainder\n        n = n//10\n\n    if start == reverse:\n        print(\"true\")\n    else:\n        print(\"false\")\n\ncheck_palindrome(n)\n</code></pre></p> </li> </ol>"},{"location":"python/OOPS/","title":"OOPS","text":"<p>Classes and functions</p>"},{"location":"python/advanced_python/","title":"Advanced","text":"<p>Patterns here. Generators here,</p>"},{"location":"python/algorithms/","title":"Algorithms","text":"<p>Has search and sorting algorithms.</p>"},{"location":"python/efficient_python/","title":"speed and memory","text":"<p>it holds memory efficient, speed efficient python code</p>"},{"location":"python/generators/","title":"Generators","text":""},{"location":"python/multithread_multiprocess/","title":"Multithread and processing","text":"<p>1. Is python multithread:</p> <p>Python is multithreaded, but not paralelly multithreaded. It means both threads can not start at same time.</p>"},{"location":"python/multithread_multiprocess/#what-is-concurrency","title":"What is concurrency?","text":"<p>Before understanding multithreading and multiprocessing, one should know about concurrency. Concurrency is Execution of Multiple tasks simultaneously.</p> <ul> <li> <p>Multithreading</p> </li> <li> <p>Multiprocessing</p> </li> <li> <p>Asyncio</p> </li> </ul> <p>All the 3 has their own advantages and disadvantages. </p>"},{"location":"python/multithread_multiprocess/#concurrency-vs-parallelsim","title":"Concurrency vs Parallelsim:","text":"<p>Running multiple tasks simultaneously is Concurrency. Multi-Threading is example of concurrency, as we run different parts of program using multiple threads simultaneously. </p> <p>Multiprocessing is example of parallelism. Multiprocessing uses multiple cores, with each core having a thread.</p> <p></p> <p>2.what is a Process:</p> <p>A process is an instance of a programme. This may be an instance of firefox, notepad, vscode. Each process will atleast have one thread, which again has own register, and stack.</p> <p></p> <p>Os can determine, when and  how long a thread can run. A process can spawn a single thread or multiple threads.</p> <p></p> <p>Data is accessible between threads.</p> <p>But Not between Process.</p> <p></p> <p>In order to share data between 2 processes, we need Queue and Pipes.</p> <p></p>"},{"location":"python/multithread_multiprocess/#3gil","title":"3.GIL","text":"<p>In order for threads to run (or) for threads safety(means no Deadlocks), One need GIL.</p>"},{"location":"python/multithread_multiprocess/#4thread-vs-process","title":"4.Thread vs Process","text":"<p>A core in CPU atleast uses a Single thread. using multiple threads for single core of a CPU is called Multithreading. </p> <p>Using multiple cores of a CPU, with each core having atleast a single thread is called multiprocessing. </p> <p>Concurrent non parallel in the above image, says that threads can not start at same time, means parallely.</p> <p>Threads have memory shared between. where as in process a seperate memory for each process is used &amp; they can be accessed with queues and pipes.(Discussed above).</p>"},{"location":"python/multithread_multiprocess/#threading-vs-processing-in-real-time","title":"Threading vs Processing in Real Time.","text":"<p>from above pictures, we see in threading, no 2 threads start at same time. also threding occurs in single core. </p> <p>where as in processing, 2 process can start at same time. each process uses a single core.</p>"},{"location":"python/multithread_multiprocess/#note","title":"Note:","text":"<p>Since both threads can not start at same time, a process can stay idle for some time overall(look above image). It means we are not utilizing it effiently.</p> <p>Where as Process can run simultaneously, we are utilizing CPU efficiently.</p>"},{"location":"python/multithread_multiprocess/#when-to-use-threading-when-to-use-processing","title":"When to use threading &amp; when to use Processing.","text":"<ul> <li>threading is suggested for I/O dependent tasks.</li> <li>processing is suggested for Compute heavy tasks.</li> </ul>"},{"location":"python/multithread_multiprocess/#threading-vs-multiprocessing","title":"Threading vs Multiprocessing:","text":"<p>you'll observe that </p> <p>threading is generally faster to start but shares the same memory space (Global Interpreter Lock or GIL) which can lead to performance degradation for CPU-bound tasks.</p> <p>Multiprocessing, on the other hand, creates separate memory space for each process, which avoids GIL-related issues but incurs more overhead due to the creation of separate processes.</p> <p>Multithreadig Example1: <pre><code>import threading\nimport time\n\ndef print_numbers():\n    for i in range(5):\n        print(\"Thread 1:\", i)\n        time.sleep(1)\n\ndef print_letters():\n    for char in 'abcde':\n        print(\"Thread 2:\", char)\n        time.sleep(1)\n\nthread1 = threading.Thread(target=print_numbers)\nthread2 = threading.Thread(target=print_letters)\n\nthread1.start()\nthread2.start()\n\nthread1.join()\nthread2.join()\n\nprint(\"Multithreading finished.\")\n</code></pre></p>"},{"location":"python/multithread_multiprocess/#what-is-start-and-join","title":"What is Start and Join","text":"<p>We start both the threads using start() method. </p> <p>join() ensures that the main thread waits for these threads to complete before terminating.</p> <p>Multithread example2: <pre><code>import threading\n\ndef compute_prime(limit):\n    primes = []\n    for num in range(2, limit + 1):\n        if all(num % i != 0 for i in range(2, int(num ** 0.5) + 1)):\n            primes.append(num)\n    print(\"Thread 1 - Prime numbers:\", primes)\n\ndef compute_factorial(num):\n    factorial = 1\n    for i in range(1, num + 1):\n        factorial *= i\n    print(\"Thread 2 - Factorial:\", factorial)\n\nthread1 = threading.Thread(target=compute_prime, args=(50,))\nthread2 = threading.Thread(target=compute_factorial, args=(5,))\n\nthread1.start()\nthread2.start()\n\nthread1.join()\nthread2.join()\n\nprint(\"Multithreading finished.\")\n</code></pre></p> <ul> <li> <p>The key advantage of multithreading is that it allows maximum utilization of a single CPU core by executing threads concurrently.</p> </li> <li> <p>All threads share same process resources like memory. </p> </li> <li> <p>Context switching between threads is lightweight.</p> </li> </ul> <p>Multiprocessing Example1: <pre><code>from multiprocessing import Process\nimport time\n\ndef print_numbers():\n    for i in range(5):\n        print(\"Process 1:\", i)\n        time.sleep(1)\n\ndef print_letters():\n    for char in 'abcde':\n        print(\"Process 2:\", char)\n        time.sleep(1)\n\nif __name__ == '__main__':\n    process1 = Process(target=print_numbers)\n    process2 = Process(target=print_letters)\n\n    process1.start()\n    process2.start()\n\n    process1.join()\n    process2.join()\n\n    print(\"Multiprocessing finished.\")\n</code></pre></p> <p>Multiprocessing Example2: <pre><code>from multiprocessing import Process\n\ndef compute_prime(limit):\n    primes = []\n    for num in range(2, limit + 1):\n        if all(num % i != 0 for i in range(2, int(num ** 0.5) + 1)):\n            primes.append(num)\n    print(\"Process 1 - Prime numbers:\", primes)\n\ndef compute_factorial(num):\n    factorial = 1\n    for i in range(1, num + 1):\n        factorial *= i\n    print(\"Process 2 - Factorial:\", factorial)\n\nif __name__ == '__main__':\n    process1 = Process(target=compute_prime, args=(50,))\n    process2 = Process(target=compute_factorial, args=(5,))\n\n    process1.start()\n    process2.start()\n\n    process1.join()\n    process2.join()\n\n    print(\"Multiprocessing finished.\")\n</code></pre></p> <p>Finally note that for multi-procesing, using all cores is very extreme. it is suggested to use only half of avaible cores in CPU. </p> <ul> <li> <p>Multiprocessing is memory overhead.</p> </li> <li> <p>Multithreading is Context-switch b/n threads &amp; synchronization.</p> </li> </ul>"},{"location":"python/property/","title":"property method","text":"<p>1.Private variable: Like in Other programming languages, python also has private variable option. Any variable that has \"_\" before the name of variable is called private variable.</p> <p>ex: <code>_my_attribute</code></p> <p>2.Property: Using <code>@property</code> decorator, we can access those private variables even outside of the Class. </p> <p>3.getter: using <code>@propery</code> on top of any method, default becomes \"Getter method\".</p> <p>4.Setter: using <code>@method.setter</code> functionality on top of a function, gives access to set a value to the private variable.</p>"},{"location":"python/property/#example","title":"Example","text":"<pre><code>class MyClass:\n    def __init__(self):\n        self._my_attribute = None  # Private attribute with leading underscore convention\n\n    # Getter method\n    @property\n    def my_attribute(self):\n        return self._my_attribute\n\n    # Setter method\n    @my_attribute.setter\n    def my_attribute(self, value):\n        self._my_attribute = value\n\n    # delter method\n    @my_attribute.deleter\n    def my_attribute(self,):\n        print(\"Deleted\")\n        del self._my_attribute\n\n# Usage\nobj = MyClass()\nobj.my_attribute = 42  # Calls the setter method\nprint(obj.my_attribute)  # Calls the getter method\ndel obj.my_attribute # deletes the variable.\n</code></pre>"},{"location":"python/static_method/","title":"static method","text":"<p>1.Static method: use when you want to denote method inside of a class as static.</p> <p>2.When to use: Static methods only belong to class, not to the instance of a class. means when you do not want to instantiate class, but want to access methods inside of them.</p> <pre><code>class MathOperations:\n    @staticmethod\n    def add(x, y):\n        return x + y\n\n    @staticmethod\n    def subtract(x, y):\n        return x - y\n\n# You can call static methods directly on the class without needing to instantiate an object\nprint(MathOperations.add(5, 3))  # Output: 8\nprint(MathOperations.subtract(5, 3))  # Output: 2\n</code></pre>"},{"location":"python/terminal_multiplexer/","title":"Tmux","text":""},{"location":"python/terminal_multiplexer/#multiplexers","title":"Multiplexers","text":"<ul> <li>Terminal multiplexers like Tmux(cheatsheet), Screen(same as tmux) are very useful to run multiple operations at same time.</li> </ul> <pre><code>tmux ls\ntumx new-session -s ray_llm for new-session\ntmux attach-session -t ray_llm for attach-session\nctrl+b d to detach session\n</code></pre>"}]}